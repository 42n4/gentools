#http://ix.io/nOt
#http://www.45drives.com/wiki/index.php?title=Installing_MATE_on_CentOS_7
yum install -y epel-release
yum groupinstall -y "X Window system"
yum groupinstall -y "MATE Desktop"
systemctl isolate graphical.target
systemctl set-default graphical.target
yum install -y gnome-disk-utility yum-utils novnc x11vnc tigervnc tigervnc-server
yum install -y sysbench iperf bonnie++ 
free && sync && echo 3 > /proc/sys/vm/drop_caches && free
sysbench --test=cpu --cpu-max-prime=10000 --num-threads=4 run
sysbench --test=fileio --file-test-mode=rndwr run
sysbench --test=fileio help
sysbench --test=fileio --file-test-mode=seqwr --num-threads=1 --file-block-size=4096 run
sysbench --test=fileio --file-test-mode=rndwr --num-threads=1 --file-block-size=4096 run
#https://wiki.mikejung.biz/Sysbench#Install_Sysbench_on_CentOS_7
#http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/
for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; 
do [ -f $CPUFREQ ] || continue; 
  echo -n performance > $CPUFREQ; 
done          
grep -E '^model name|^cpu MHz' /proc/cpuinfo    
#http://cromwell-intl.com/linux/performance-tuning/disks.html
iperf -s #na jednym
iperf -c server03 -i1 -t 10  #na drugim
echo 3 | sudo tee /proc/sys/vm/drop_caches && sudo sync
bonnie++ -s 8192 -r 4096 -u root -d /mnt/ -m BenchClient
curl ix.io/client > /usr/local/bin/ix
chmod +x /usr/local/bin/ix



#CEPH
#http://www.virtualtothecore.com/en/adventures-ceph-storage-part-1-introduction/
#https://blog.zhaw.ch/icclab/tag/ceph/
#https://wiki.centos.org/SpecialInterestGroup/Storage/ceph-Quickstart
#http://linoxide.com/storage/setup-red-hat-ceph-storage-centos-7-0/
#http://karan-mj.blogspot.com/2013/12/what-is-ceph-ceph-is-open-source.html
#https://www.reddit.com/r/DataHoarder/comments/4gzpxi/why_is_ceph_so_rare_for_home_use_even_among/
#http://palmerville.github.io/2016/04/30/single-node-ceph-install.html
cat <<__EOF__ >> ./ceph.conf
mon_pg_warn_max_per_osd = 0
public network = 192.168.10.0/24
cluster network = 10.10.10.0/24
#Choose reasonable numbers for number of replicas and placement groups.
osd pool default size = 2 # Write an object 2 times
osd pool default min size = 1 # Allow writing 1 copy in a degraded state
osd pool default pg num = 256
osd pool default pgp num = 256
#Choose a reasonable crush leaf type
#0 for a 1-node cluster.
#1 for a multi node cluster in a single rack
#2 for a multi node, multi chassis cluster with multiple hosts in a chassis
#3 for a multi node cluster with hosts across racks, etc.
osd crush chooseleaf type = 1
[osd.0]
public addr = 192.168.10.53
cluster addr = 10.10.10.13
host = osd3
devs = /dev/sdb
osd journal = /dev/sda2
[osd.1]
public addr = 192.168.10.54
cluster addr = 10.10.10.14
host = osd4
devs = /dev/sdb
osd journal = /dev/sda2
[osd.2]
public addr = 192.168.10.56
cluster addr = 10.10.10.16
host = osd6
devs = /dev/sdb
osd journal = /dev/sda2
[osd.3]
public addr = 192.168.10.58
cluster addr = 10.10.10.18
host = osd8
devs = /dev/sdb
osd journal = /dev/sda2
__EOF__

 
cat <<__EOF__ > /usr/local/bin/init_ceph
server="xenserverhw0"
host01="3"
host02="4"
host03="6"
[ ! -d ceph-deploy ] && mkdir ceph-deploy
cd ceph-deploy/
ceph-deploy purge xenserverhw03 xenserverhw04 xenserverhw06
ceph-deploy purgedata xenserverhw03 xenserverhw04 xenserverhw06
ceph-deploy forgetkeys
ceph-deploy new xenserverhw03 xenserverhw04 xenserverhw06
#ceph-deploy install --release jewel --no-adjust-repos xenserverhw03 xenserverhw04 xenserverhw06
ceph-deploy install --release jewel xenserverhw03 xenserverhw04 xenserverhw06
#ceph-deploy install --repo-url http://download.ceph.com/rpm-jewel/el7/ xenserverhw03 xenserverhw04 xenserverhw06
ceph-deploy --overwrite-conf mon create-initial

ceph-deploy gatherkeys xenserverhw03
for i in osd3 osd4 osd6; do ceph-deploy disk zap $i:sdb; done
ae "parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%"
ae "mkfs.xfs /dev/sdb1 -f"
#http://tracker.ceph.com/issues/13833
ae "chown ceph:ceph /dev/sda2"
for i in osd3 osd4 osd6; do 
ceph-deploy osd prepare $i:/dev/sdb1:/dev/sda2; done
for i in osd3 osd4 osd6; do 
ceph-deploy osd activate $i:/dev/sdb1:/dev/sda2; done
#ceph-deploy  --username ceph osd create osd3:/dev/sdb1:/dev/sda2
ceph-deploy admin xenserverhw03 xenserverhw04 xenserverhw06 osd3 osd4 osd6
ae "chmod +r /etc/ceph/ceph.client.admin.keyring"
ae "systemctl enable ceph-mon.target"
ae "systemctl enable ceph-mds.target"
ae "systemctl enable ceph-osd.target"
#object storage gateway
ceph-deploy rgw create xenserverhw03 xenserverhw04 xenserverhw06
#cephfs
ceph-deploy mds create xenserverhw03 xenserverhw04 xenserverhw06
ceph -s #ceph status
ceph osd tree
ceph mon_status
ceph osd pool create mypool 1
ceph osd lspools
ceph df
echo "test data" > testfile
rados put -p mypool testfile testfile
rados -p mypool ls
rados -p mypool setomapval testfile mykey myvalue
rados -p mypool getomapval testfile mykey
rados get -p mypool testfile testfile2
md5sum testfile testfile2 

ceph osd pool create cephfs_data 128
ceph osd pool create cephfs_metadata 128
ceph fs new cephfs cephfs_metadata cephfs_data
mkdir /mnt/mycephfs
mount -t ceph xenserverhw03:6789:/ /mnt/mycephfs -o name=admin,secret=`cat ./ceph.client.admin.keyring | grep key | cut -f 2 | sed 's/key = //g'`
__EOF__
